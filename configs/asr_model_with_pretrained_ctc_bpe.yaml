model:
  sample_rate: 16000
  pretrained_model: ??? # pretrained encoder used for feature extractor
  use_pretrained: True # use pretrained
  use_nemo: False # use nemo toolkit

  # nemo toolkit - utilize nemo toolkit alter architecture
  nemo:
    encoder: # model encoder - customized
      d_model:
      self_attention_model: rel_pos_local_attn
      att_context_size: [2, 2]
      att_context_style: regular # regular or chunked_limited
      xscaling: true # scales up the input embeddings by sqrt(d_model)
      untie_biases: true # unties the biases of the TransformerXL layers
      pos_emb_max_len: 5000

  processor:
    sample_rate: 16000
    n_fft: 640
    win_length: 640
    hop_length: 321
    window_fn: "torch.hamming_window"
    center: True
    pad_mode: "reflect"
    power: 2.0
    norm: "slaney"
    n_mels: 80 # mel banks default n_mels
    mel_scale: "slaney"
    top_db: 25
    pshift_bins_per_octave: 12
    pshift_steps: 4

  log:
    dir: '../core/logger/logs/model.log'

trainer:
  max_epochs: 100
